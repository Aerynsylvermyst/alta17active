{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Live shared task\n",
    "\n",
    "The challenge is to build a sentence-level classifier for identyfing [adverse drug events](https://en.wikipedia.org/wiki/Adverse_event) in 60 minutes. You are free to use any data and annotation strategy you think best trades off hacking and labelling. Just please don't look at the test data.\n",
    "\n",
    "Some strategies to consider:\n",
    "* Get started with random or query-driven sampling.\n",
    "* Use the dev data for seeding learning instead of generalisation testing and analysis.\n",
    "* Tune classifier choice, hyperparameters or feature extraction.\n",
    "* Use error analysis over the dev data to refine your strategy.\n",
    "* Active learning by uncertainty or ensembles.\n",
    "* Collect 10 or more query functions and use as snorkel labelling functions.\n",
    "* Find additional data, e.g., [Twitter](https://archive.org/details/twitterstream).\n",
    "* Interactive web search or [Reddit queries](http://minimaxir.com/2015/10/reddit-bigquery/).\n",
    "* Use external data (e.g., [MAUDE](https://www.fda.gov/MedicalDevices/DeviceRegulationandGuidance/PostmarketRequirements/ReportingAdverseEvents/ucm127891.htm)) for querying or labelling functions.\n",
    "\n",
    "Please don't use data from the following as they are sources of our held-out data:\n",
    "* CSIRO CADEC data set\n",
    "* AskaPatient\n",
    "* DIEGO Lab Twitter data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dev data\n",
    "from dataset import Dataset\n",
    "dev = Dataset.from_csv('../shared-task/dev.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pool data\n",
    "\n",
    "Now let's load the unlabelled pool data. We have data from several sources:\n",
    "* `aska` - Posts for additional drugs from AskaPatient\n",
    "* `ader` - Comments mentioning the same drugs from Reddit\n",
    "* `adeb` - Tweets mentioning the same set of drugs\n",
    "* `adrc` - Tweets mentioning an overlapping set of drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load unlabelled data pools\n",
    "aska = Dataset.from_csv('../shared-task/aska.csv')\n",
    "# ader = Dataset.from_csv('../shared-task/ader.csv')\n",
    "# adeb = Dataset.from_csv('../shared-task/adeb.csv')\n",
    "adrc = Dataset.from_csv('../shared-task/adrc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{None: 3761}\n",
      "{None: 14712}\n",
      "{None: 4409}\n"
     ]
    }
   ],
   "source": [
    "#dev.label_distribution\n",
    "print(dev.label_distribution)\n",
    "print(aska.label_distribution)\n",
    "print(adrc.label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wradford/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "768\tI\n",
      "483\tpain\n",
      "209\tmuscle\n",
      "106\tback\n",
      "99\tLipitor\n",
      "98\tloss\n",
      "97\ttaking\n",
      "90\tsevere\n",
      "88\tcramps\n",
      "85\tlegs\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "en = set(stopwords.words('english'))\n",
    "\n",
    "unigrams = Counter()\n",
    "for text, label in dev.oracle_items:\n",
    "    if label:\n",
    "        tokens = word_tokenize(text)\n",
    "        # print(label, tokens)\n",
    "        unigrams.update(t for t in tokens \n",
    "                        if not t in en \n",
    "                        and not t in set(string.punctuation))\n",
    "for t, count in unigrams.most_common(10):\n",
    "    print('{}\\t{}'.format(count, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db59ae2c0314415486e264ddc2e36592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>VBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "VBox(children=(HBox(children=(Button(description='Yes', style=ButtonStyle()), Button(description='No', style=ButtonStyle()))), HTML(value='<p>I first thought this drug was a Godsend despite severe insomnia which lasted the first 3 months (I had never suffered from it before and then I was waking up around 3 AM every night).</p>')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from annotator import AnnotationPane\n",
    "from samplers import Random\n",
    "\n",
    "def mentions_top_ten(item):\n",
    "    return bool(re.search(r'\\b(I|pain|muscle|back|Lipitor|loss|taking|severe|cramps|legs)\\b', item[0], flags=re.IGNORECASE))\n",
    "query_sampler = Random(None, batch_size=30, query=mentions_top_ten)\n",
    "\n",
    "# annotate\n",
    "pane = AnnotationPane(aska, query_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{False: 41, None: 14652, True: 19}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aska.label_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data programming \n",
    "\n",
    "One view of data programming is that it takes the query functions we used in the previous exercise and uses them for weak supervision. It does this by pooling labelling function output using weighted voting.\n",
    "\n",
    "A simple implementation could use the inter-annotator agreement scripts from exercise 1.1 to weight each labelling function by its average agreement score.\n",
    "\n",
    "In the setting here, where we have dev data, we could also weight each labelling function by its perforamance on the labelled dev data. Of course, this wouldn't work in an annotation setting where we were starting without labelled data.\n",
    "\n",
    "A key difference with `snorkel` is that this approach in the annotation framework does not go on to train the classifier on a continuous voting confidence value.\n",
    "\n",
    "Feel free to experiment with voting, or use `snorkel` directly. If you do plan to use `snorkel`, note that it takes a while to [install](https://github.com/HazyResearch/snorkel#installation). It would be a good idea to run the installation in the background while you start annotating and/or writing labelling functions.\n",
    "\n",
    "Once `snorkel` is installed, the tutorials should help get things up and running. These are in the repo and can also be viewed [on github](https://github.com/HazyResearch/snorkel/tree/master/tutorials/intro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping up.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short strategy description\n",
    "\n",
    "Before submitting, please summarise:\n",
    "* The hacking/labelling strategy you followed\n",
    "* How do you rate this strategy? Why?\n",
    "\n",
    "TODO Add your summary right here.\n",
    "\n",
    "TODO If you have a list sampling strategies, please include it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Submit your annotation and system output for scoring.\n",
    "\n",
    "* Union of labels across all sets (except dev).\n",
    "* Predict dev\n",
    "* Predict test\n",
    "\n",
    "### Step 1: setup your submission directory\n",
    "\n",
    "This will use the environment `USER`, but feel free to choose your own name.\n",
    "```bash\n",
    "mkdir -p ../submissions/$USER\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ../submissions/$USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Collate your labels; train a classifier; label dev/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{None: 14652, False: 41, True: 19}\n",
      "Written submission to ../submissions/wradford/None.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from evaluation import label_for_submission\n",
    "\n",
    "USERNAME = os.environ.get('USER', 'username')\n",
    "\n",
    "# use multinomial NB again\n",
    "pipeline = Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')),\n",
    "        ('clf', MultinomialNB(alpha=.01)),\n",
    "    ])\n",
    "\n",
    "# Collate all labels.\n",
    "train = Dataset()\n",
    "# FIXME Add all the datasets.\n",
    "for d in [aska]:\n",
    "    train.update(d)\n",
    "print(train.label_distribution)\n",
    "train.to_csv('../submissions/{}/train.csv'.format(USERNAME))\n",
    "\n",
    "# Train a classifier.\n",
    "X_train, y_train = zip(*train.labelled_items)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "label_for_submission(dev, pipeline, 'dev', USERNAME)\n",
    "# FIXME Once test is available, use it.\n",
    "# label_for_submission(test, pipeline, 'test', USERNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: save this notebook and copy to the submission directory\n",
    "\n",
    "First click `<ctrl>+<s>` to save, then run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy your notebook to your submission directory\n",
    "! cp exercise_2.ipynb ../submissions/$USER/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push your submission back to the repo\n",
    "! git add ../submissions/$USER\n",
    "! git commit -m 'Checkpoint $USER' ../submissions/$USER/\n",
    "! git push"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
