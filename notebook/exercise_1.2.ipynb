{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.2: Active learning\n",
    "\n",
    "## Data\n",
    "\n",
    "First let's load our newsgroup guns/no-guns data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "def guns_dataset_factory(subset='train', labelled=False):\n",
    "    \"\"\" Fetches newsgroup data and returns a Dataset. \"\"\"\n",
    "    newsgroups = fetch_20newsgroups(subset=subset)\n",
    "    \n",
    "    # Transform to guns or not.\n",
    "    labels = {i: name == 'talk.politics.guns' for i, name in enumerate(newsgroups.target_names)}\n",
    "    dataset = Dataset({text: labels[i] for text, i in zip(newsgroups.data, newsgroups.target)})\n",
    "    return dataset\n",
    "\n",
    "pool = guns_dataset_factory(subset='train')\n",
    "test = guns_dataset_factory(subset='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling by query\n",
    "\n",
    "In addition to a pipeline and batch_size, `Sampler` can take two filter function arguments:\n",
    "* `query` - a function to filter before prediction\n",
    "* `key` - a key function to sort predictions\n",
    "* `accept` - a function to filter predictions\n",
    "\n",
    "The `query` filter can be used to sample by keyword, e.g., search for examples containing the word gun. We'll cover `accept` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from samplers import Random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "\n",
    "# use multinomial NB again\n",
    "pipeline = Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')),\n",
    "        ('clf', MultinomialNB(alpha=.01)),\n",
    "    ])\n",
    "\n",
    "# set up a random sampler with a query filter that matches examples containing the word gun\n",
    "def mentions_gun(item):\n",
    "    return bool(re.search(r'\\bgun\\b', item[0], flags=re.IGNORECASE))\n",
    "query_sampler = Random(pipeline, batch_size=10, query=mentions_gun)\n",
    "\n",
    "# sample \n",
    "for i, (text, label) in enumerate(query_sampler(pool)):\n",
    "    print(i, label, repr(text[:80]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A selective sampler\n",
    "\n",
    "Here is a straw man active sampler that:\n",
    "* trains a classifier on the labelled data\n",
    "* predicts the labels of unlabelled data\n",
    "* selects the first n examples with a specific label profile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from samplers import Active\n",
    "\n",
    "# suppress sklearn FutureWarnings in terminal output\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# set up the active sampler to select uncertain examples\n",
    "def accept_uncertain(pred):\n",
    "    \" Accepts predictions within 0.1 on either side of the decision boundary. \"\n",
    "    return abs(pred[True] - 0.5) < 0.167\n",
    "active_sampler = Active(pipeline, batch_size=10, accept=accept_uncertain)\n",
    "\n",
    "# seed pool with some random labelled examples for initial classifier\n",
    "p2 = pool.copy\n",
    "p2.seed(200)\n",
    "active_sampler.fit(p2)\n",
    "\n",
    "# sample \n",
    "for i, (text, pred) in enumerate(active_sampler(p2)):\n",
    "    print(i, pred[True], repr(text[:60]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling by uncertainty\n",
    "\n",
    "Here is another active learner. This one selects examples in order according to distance of the predicted probability from the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the active sampler to select uncertain examples\n",
    "def uncertainty_sort_key(item):\n",
    "    pred = dict(zip(pipeline.classes_, item[0]))\n",
    "    return abs(pred[True] - 0.5)\n",
    "active_sampler = Active(pipeline, batch_size=10, key=uncertainty_sort_key)\n",
    "\n",
    "# seed pool with some random labelled examples for initial classifier\n",
    "p2 = pool.copy\n",
    "p2.seed(200)\n",
    "active_sampler.fit(p2)\n",
    "\n",
    "# sample \n",
    "for i, (text, pred) in enumerate(active_sampler(p2)):\n",
    "    print(i, pred[True], repr(text[:60]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation with bootstrap resampling\n",
    "\n",
    "Let's just use bootstrap resampling this time. It's less reliable, but it's fast.\n",
    "\n",
    "How does the active test curve compare to the random curve from exercise 1.1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import run_bootstraps\n",
    "from evaluation import plot_learning_curve\n",
    "import numpy as np\n",
    "\n",
    "# run a simulated experiment and plot learning curve\n",
    "active_sampler = Active(pipeline, batch_size=2000, key=uncertainty_sort_key)\n",
    "train_sizes, train_scores, test_scores = run_bootstraps(active_sampler, pool.copy, test)\n",
    "\n",
    "# plot learning curve\n",
    "plt = plot_learning_curve(np.asarray(train_sizes), np.asarray(train_scores), np.asarray(test_scores))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other sampling strategies\n",
    "\n",
    "Feel free to implement other strategies, e.g.:\n",
    "* ensemble sampling with random forests,\n",
    "* ensemble sampling with generative versus discriminative classifiers,\n",
    "* ensemble sampling with subject versus body features.\n",
    "\n",
    "What is the speed-accuracy tradeoff for these approaches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation\n",
    "\n",
    "Labelling examples now, we should notice that more are relevant and/or confusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annotator import AnnotationPane\n",
    "\n",
    "# let's use the sampler that returns predictions within 0.167 of the decision boundary\n",
    "active_sampler = Active(pipeline, batch_size=10, accept=accept_uncertain)\n",
    "\n",
    "# seed pool with some random labelled examples for initial classifier\n",
    "pool.seed(200)\n",
    "active_sampler.fit(p2)\n",
    "\n",
    "# annotate\n",
    "pane = AnnotationPane(pool, active_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
