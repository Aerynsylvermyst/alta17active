{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring\n",
    "\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from dataset import Dataset\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "LABELS = {True, False}\n",
    "\n",
    "\n",
    "def get_labels(text_to_label):\n",
    "    return list(i[1] for i in sorted(text_to_label))\n",
    "\n",
    "gold_dev = Dataset.from_csv('../shared-task/dev.csv')\n",
    "if os.path.exists('../shared-task/test.csv'):\n",
    "    gold_test = Dataset.from_csv('../shared-task/test.csv')\n",
    "else:\n",
    "    gold_test = None\n",
    "\n",
    "    \n",
    "submissions = []\n",
    "datasets = {}\n",
    "for sub_dir in glob.glob('../submissions/*'):\n",
    "    data = {\n",
    "        'username': os.path.basename(sub_dir),\n",
    "    }\n",
    "    print('Reading {}'.format(sub_dir))\n",
    "    path = os.path.join(sub_dir, 'train.csv')\n",
    "    if os.path.exists(path):\n",
    "        train = Dataset.from_csv(path)\n",
    "        data['train_n'] = len(train)\n",
    "        for label, count in train.label_distribution.items():\n",
    "            if label not in LABELS:\n",
    "                print('\\tSkipping label {} (count={})'.format(label, count))\n",
    "            else:\n",
    "                data['train_{}'.format(label)] = count\n",
    "    for fname, gold in [('dev.csv', gold_dev), ('test.csv', gold_test)]:\n",
    "        # Prepare gold text and labels.\n",
    "        gold_text = set(i[0] for i in gold.oracle_items)\n",
    "        gold_labels = get_labels(gold.oracle_items)\n",
    "        \n",
    "        # Load system data.\n",
    "        split = fname.split('.')[0]\n",
    "        path = os.path.join(sub_dir, fname)\n",
    "        if os.path.exists(path):\n",
    "            system = Dataset.from_csv(path)\n",
    "        else:\n",
    "            system = None\n",
    "        \n",
    "        if not system or not gold:\n",
    "            print('\\tSkipping {}'.format(split))\n",
    "            continue\n",
    "        else:\n",
    "            print('\\tEvaluating {}\\t{}\\t{}'.format(split, len(gold), len(system)))\n",
    "        \n",
    "        # Check the data is the same set.\n",
    "        system_text = set(i[0] for i in system.oracle_items)\n",
    "        assert system_text == gold_text\n",
    "        system_labels = get_labels(system.oracle_items)\n",
    "        if not set(system_labels).issubset(LABELS):\n",
    "            print('\\tSkipping incomplete system labels {}'.format(set(system_labels)))\n",
    "            continue\n",
    "        if not set(gold_labels).issubset(LABELS):\n",
    "            print('\\tSkipping incomplete gold labels {}'.format(set(gold_labels)))\n",
    "            continue\n",
    "        \n",
    "        # Calculate and add metrics.\n",
    "        p, r, f, s = precision_recall_fscore_support(\n",
    "            gold_labels,\n",
    "            system_labels,\n",
    "        )\n",
    "        for key, (value_false, value_true) in zip('prf', (p, r, f)):\n",
    "            data['{}_{}_{}'.format(split, key, False)] = value_false\n",
    "            data['{}_{}_{}'.format(split, key, True)] = value_true\n",
    "        p, r, f, s = precision_recall_fscore_support(\n",
    "            gold_labels,\n",
    "            system_labels,\n",
    "            average='micro',\n",
    "        )\n",
    "        for key, value in zip('prf', (p, r, f)):\n",
    "            data['{}_{}_micro'.format(split, key)] = value\n",
    "        submissions.append(data)\n",
    "        \n",
    "        datasets.setdefault(split, {})[data['username']] = system\n",
    "        \n",
    "cols = [\n",
    "  'dev_p_False',\n",
    "  'dev_r_False',\n",
    "  'dev_f_False',\n",
    "  'dev_p_True',\n",
    "  'dev_r_True',\n",
    "  'dev_f_True',\n",
    "  'dev_p_micro',\n",
    "  'dev_r_micro',\n",
    "  'dev_f_micro',\n",
    "]\n",
    "cols.extend([l.replace('dev_', 'test_') for l in cols])\n",
    "\n",
    "df = pd.DataFrame(submissions,\n",
    "                  columns=[\n",
    "                      'username',\n",
    "                      'train_n',\n",
    "                      'train_True',\n",
    "                      'train_False',\n",
    "                  ] + cols)\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissions ordered by micro-averaged F1 on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('dev_f_micro', ascending=False)[['username', 'dev_p_micro', \n",
    "                                                'dev_r_micro', 'dev_f_micro']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "pairs = {}\n",
    "for i in users:\n",
    "    pairs[i] = {j: 0 for j in users}\n",
    "\n",
    "for i, j in itertools.combinations(users, 2):\n",
    "    print(i, j)\n",
    "    a = accuracy_score(\n",
    "        get_labels(datasets['dev'][i].oracle_items),\n",
    "        get_labels(datasets['dev'][j].oracle_items),\n",
    "    )\n",
    "    pairs[i][j] = a\n",
    "    pairs[j][i] = a\n",
    "\n",
    "flat = []\n",
    "for i, j_a in pairs.items():\n",
    "    for j, a in j_a.items():\n",
    "        flat.append({\n",
    "            'i': i,\n",
    "            'j': j,\n",
    "            'a': a,\n",
    "        })\n",
    "        \n",
    "sns.heatmap(pd.DataFrame(flat).pivot('i', 'j', 'a'),\n",
    "            annot=True,\n",
    "            linewidths=1,\n",
    "            square=True,\n",
    "            cmap=sns.color_palette(\"BuGn_r\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
