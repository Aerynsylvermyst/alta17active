{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataDataData\n",
    "\n",
    "Here's a sample class wrapping the main abstraction, the `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, text_to_label=None):\n",
    "        self.text_to_label = text_to_label or {}\n",
    "        \n",
    "    def add_label(self, text, label):\n",
    "        self.text_to_label[text] = label\n",
    "        \n",
    "    def iter_texts(self, accept=lambda label: True):\n",
    "        return ((text, label) for text, label in \n",
    "                 self.text_to_label.items() if accept(label))\n",
    "    \n",
    "    def to_csv(self, fname):\n",
    "        with open(fname, 'w') as f:\n",
    "            w = csv.writer(f, delimiter=',')\n",
    "            for text, label in self.text_to_label.items():\n",
    "                w.writerow((label, text))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_csv(cls, fname):\n",
    "        with open(fname) as f:\n",
    "            return cls({text: label for text, label in\n",
    "                        csv.reader(f, delimiter=',')})\n",
    "        \n",
    "    def update(self, other):\n",
    "        self.text_to_label.update(other.text_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 newgroups\n",
    "\n",
    "* Take the training set.\n",
    "* Remove every second label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "def build_newsgroups(subset='train'):\n",
    "    newsgroups_train = fetch_20newsgroups(subset=subset)\n",
    "    label_names = {index: name for index, name in \n",
    "                   enumerate(newsgroups_train.target_names)}\n",
    "    return Dataset({text: label_names[index] \n",
    "                    for text, index in zip(newsgroups_train.data, \n",
    "                                           newsgroups_train.target)})\n",
    "\n",
    "def unlabel(dataset, p=0.5):\n",
    "    for text, label in dataset.iter_texts():\n",
    "        if random.random() > p:\n",
    "            dataset.add_label(text, None)\n",
    "\n",
    "\n",
    "train = build_newsgroups()\n",
    "unlabel(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active sampling with classifier\n",
    "\n",
    "Here is a straw man active sampler that:\n",
    "* trains a classifier on the labelled data\n",
    "* predicts the labels of unlabelled data\n",
    "* selects text with a specific label profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def iter_active_sample(dataset, cross_validate=True, accept=lambda i: True):\n",
    "    X, y = [], []\n",
    "    for text, label in list(dataset.iter_texts(lambda l: l is not None)):\n",
    "        X.append(text)\n",
    "        y.append(label)\n",
    "    print('Got {} labelled samples'.format(len(X)))\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer()),\n",
    "        ('clf', SGDClassifier(loss='log')),\n",
    "    ])\n",
    "    if cross_validate:\n",
    "        print('Cross-validating')\n",
    "        scores = cross_val_score(pipeline, X, y, cv=3)\n",
    "        print(\"Cross-validated accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    print('Refitting')\n",
    "    pipeline.fit(X, y)\n",
    "    X, y = [], []\n",
    "    for text, label in list(dataset.iter_texts(lambda l: l is None)):\n",
    "        X.append(text)\n",
    "        y.append(label)\n",
    "    print('Predicting {} unlabelled'.format(len(X)))\n",
    "    for probs, text in zip(pipeline.predict_proba(X), X):\n",
    "        predictions = dict(zip(pipeline.classes_, probs))\n",
    "        if accept(predictions):\n",
    "            yield (predictions, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 5569 labelled samples\n",
      "Refitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 5745 unlabelled\n",
      "{'alt.atheism': 3.7254615575514024e-132, 'comp.graphics': 5.9078108888745949e-39, 'comp.os.ms-windows.misc': 0.50000107012676465, 'comp.sys.ibm.pc.hardware': 1.7130306339580281e-17, 'comp.sys.mac.hardware': 9.1751144692482513e-25, 'comp.windows.x': 1.5612169217704451e-46, 'misc.forsale': 3.7149814655818378e-17, 'rec.autos': 9.5618936804095389e-12, 'rec.motorcycles': 1.2703873625525703e-47, 'rec.sport.baseball': 0.49999892986367345, 'rec.sport.hockey': 2.1858647319627581e-34, 'sci.crypt': 2.894246204064473e-61, 'sci.electronics': 3.6691150545235971e-23, 'sci.med': 1.8785525903306693e-35, 'sci.space': 1.7350890327670784e-45, 'soc.religion.christian': 6.2996141248465761e-99, 'talk.politics.guns': 9.304136797469697e-89, 'talk.politics.mideast': 2.1207557875253542e-76, 'talk.politics.misc': 3.237194377273919e-95, 'talk.religion.misc': 8.71616735528201e-138}\t'From: vince@sscl.uwo.ca\\nSubject: Re: Early BBDDD Returns?\\nOrganization: Social Science Computing Laboratory\\nNntp-Posting-Host: vaxi.sscl.uwo.ca\\nLines: 11\\n\\nIn article <1993Apr16.073051.9160@news.cs.brandeis.edu>, st902415@pip.cc.brandeis.edu (Adam Levin) writes:\\n> Just curious if anyone has started to standout early in the season in the\\n> BB DDD this year. ...\\n> \\n> A concerned fan of the BB DDD,\\n\\nI am hoping to produce the first update of the BB DDD this week;\\nplease send info about the most significant (longest, most critical,\\netc.) home run that you have seen yet this season.\\n\\nVince.\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/base.py:347: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n"
     ]
    }
   ],
   "source": [
    "for preds, X in iter_active_sample(train, \n",
    "                                   cross_validate=False, \n",
    "                                   accept=lambda preds: max(preds.values()) < 0.7):\n",
    "    print('{}\\t{}'.format(preds, repr(text)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data programming\n",
    "\n",
    "We can weakly supervise using precise functions. Note that we have no fancy model above the labelling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sport(text):\n",
    "    if 'puck' in text.lower():\n",
    "        return 'rec.sport.hockey'\n",
    "    elif 'home run' in text.lower():\n",
    "        return 'rec.sport.baseball'\n",
    "\n",
    "\n",
    "def iter_apply_funcs(dataset, funcs):\n",
    "    for text, _ in dataset.iter_texts(accept=lambda l: l is None):\n",
    "        for func in funcs:\n",
    "            label = func(text)\n",
    "            if label:\n",
    "                yield label, text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.sport.baseball\t'From: vince@sscl.uwo.ca\\nSubject: Re: Early BBDDD Returns?\\nOrganization: Social Science Computing Laboratory\\nNntp-Posting-Host: vaxi.sscl.uwo.ca\\nLines: 11\\n\\nIn article <1993Apr16.073051.9160@news.cs.brandeis.edu>, st902415@pip.cc.brandeis.edu (Adam Levin) writes:\\n> Just curious if anyone has started to standout early in the season in the\\n> BB DDD this year. ...\\n> \\n> A concerned fan of the BB DDD,\\n\\nI am hoping to produce the first update of the BB DDD this week;\\nplease send info about the most significant (longest, most critical,\\netc.) home run that you have seen yet this season.\\n\\nVince.\\n'\n"
     ]
    }
   ],
   "source": [
    "for label, text in iter_apply_funcs(train, [sport]):\n",
    "    print('{}\\t{}'.format(label, repr(text)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
