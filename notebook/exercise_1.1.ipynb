{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 1.1: Annotation & reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Here's a sample class wrapping the main abstraction, the `Dataset`.\n",
    "\n",
    "The following utility reads the 20 newsgroups data into a Dataset object. It sets the label to True if a message comes from the talk.politics.guns group, and to False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "def guns_dataset_factory(subset='train', labelled=False):\n",
    "    \"\"\" Fetches newsgroup data and returns a Dataset. \"\"\"\n",
    "    newsgroups = fetch_20newsgroups(subset=subset)\n",
    "    \n",
    "    # Transform to guns or not.\n",
    "    labels = {i: name == 'talk.politics.guns' for i, name in enumerate(newsgroups.target_names)}\n",
    "    dataset = Dataset({text: labels[i] for text, i in zip(newsgroups.data, newsgroups.target)})\n",
    "    return dataset\n",
    "\n",
    "pool = guns_dataset_factory(subset='train')\n",
    "test = guns_dataset_factory(subset='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A random sampler\n",
    "\n",
    "We need a way of choosing which data to annotate next. Let's start with a random sampler. This is how most crowd annotation is set up.\n",
    "\n",
    "Our `Sampler` base clase includes some utilities for sampling, training and scoring. Other samplers inherit from `Sampler` and must implement the `__call__` method, which takes one argument (a dataset).\n",
    "\n",
    "`Sampler` objects must be initialised with a sklearn classifier or pipeline. We use multinomial naive Bayes as a baseline since it is [fast to train and can achieve competitive accuracy](http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html).\n",
    "\n",
    "Our `Random` sampler simply shuffles the unlabelled data, then returns the first `batch_size` items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from samplers import Random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')),\n",
    "        ('clf', MultinomialNB(alpha=.01)),\n",
    "    ])\n",
    "\n",
    "random_sampler = Random(pipeline, batch_size=10)\n",
    "for i, (text, label) in enumerate(random_sampler(pool)):\n",
    "    print(i, label, repr(text[:80]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated experiments\n",
    "\n",
    "Now we can run simulated experiments. `run_simulation` takes a sampler and runs `n` complete simulations. It returns:\n",
    "* `train_sizes` - the training set sizes for each simulation\n",
    "* `train_scores` - f1 scores over the training set\n",
    "* `test_scores` - f1 scores over the test set\n",
    "\n",
    "Each is a list of tuples, with one tuple per iteration of sampling. Each tuple contains `n` values, one per simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from evaluation import run_simulations\n",
    "\n",
    "# suppress sklearn FutureWarnings in terminal output\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# run a simulated experiment and plot learning curve\n",
    "random_sampler = Random(pipeline, batch_size=2000)\n",
    "train_sizes, train_scores, test_scores = run_simulations(random_sampler, pool, test, n=3)\n",
    "\n",
    "print(train_sizes)\n",
    "print(train_scores)\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves\n",
    "\n",
    "Plotting performance against the number of examples gives a quick visual indication of whether more annotation will help.\n",
    "\n",
    "Here, the lower, green line corresponds to the test f1 score. This is our estimate generalisation to unseen data, our best guess of live system performance.\n",
    "\n",
    "When test performance flattens out, more labelled data probably won't help with this classifier.\n",
    "\n",
    "The plot also shows variance that our performance estimate has more variance with fewer samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import plot_learning_curve\n",
    "import numpy as np\n",
    "\n",
    "plt = plot_learning_curve(np.asarray(train_sizes), np.asarray(train_scores), np.asarray(test_scores))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation with bootstrap resampling\n",
    "\n",
    "In a setting where sampled batches were actually being annotated, we wouldn't run complete simulations.\n",
    "\n",
    "Instead, we can estimate variance using bootstrap resampling of training data in a given round of sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import run_bootstraps\n",
    "\n",
    "# run a simulated experiment and plot learning curve\n",
    "random_sampler = Random(pipeline, batch_size=2000)\n",
    "train_sizes, train_scores, test_scores = run_bootstraps(random_sampler, pool.copy, test)\n",
    "\n",
    "print(train_sizes)\n",
    "print(train_scores)\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = plot_learning_curve(np.asarray(train_sizes), np.asarray(train_scores), np.asarray(test_scores))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually label some examples\n",
    "\n",
    "Ultimately, we need labelled data - as much labelled data as possible. Sometimes we have labelled data, e.g., captured from user activity on large platforms. Sometimes we need to annotate.\n",
    "\n",
    "The `AnnotationPane` class below renders an annotation interface using iPython widgets. It takes two arguments: a Dataset object and a Sampler object. It displays examples selected from the dataset by the sampler.\n",
    "\n",
    "Try it out. Clicking the Yes button saves a True label, clicking No saves False, clicking Skip saves None which is equivalent to leaving the example unlabelled.\n",
    "\n",
    "Here we are answering the question: Does the message come from the `talk.politics.guns` newsgroup?\n",
    "\n",
    "After finishing a batch, run the cell again to get sample more examples for annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annotator import AnnotationPane\n",
    "\n",
    "pane = AnnotationPane(pool, Random(None, batch_size=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See our new labels in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pool.label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, label in pool.labelled_items:\n",
    "    print(label, repr(text[:80]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-annotator agreement\n",
    "\n",
    "Now let's have a play with inter-annotator agreement on [a doubly-labelled data set](artstein_poesio_example.txt) from Artstein and Poesio (2007), [Inter-Coder Agreement for Computational Linguistics](http://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2)\n",
    "\n",
    "First we'll visualise the label distributions for each annotator and the confusion matrix. The distributions suggest different biases per annotator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Read in the data (annotator_id, instance_id, label)\n",
    "data = [x.split() for x in open(\"artstein_poesio_example.txt\")]\n",
    "\n",
    "def filter_by_annotator(annot_id, data):\n",
    "    for annotator_id, instance_id, label in data:\n",
    "        if annotator_id == annot_id:\n",
    "            yield annotator_id, instance_id, label\n",
    "\n",
    "# sanity check that we get the same instances for a and b\n",
    "a_instances = set([instance_id for _,instance_id,_ in filter_by_annotator('b', data)])\n",
    "b_instances = set([instance_id for _,instance_id,_ in filter_by_annotator('a', data)])\n",
    "assert a_instances == b_instances\n",
    "\n",
    "# get labels for a and b\n",
    "a_labels = [label for _,_,label in sorted(filter_by_annotator('a', data))]\n",
    "b_labels = [label for _,_,label in sorted(filter_by_annotator('b', data))]\n",
    "all_labels = list(set(a_labels).union(b_labels))\n",
    "\n",
    "# view bar charts\n",
    "def make_bar_chart(data, x_labels, y_min, y_max, title):\n",
    "    c = Counter(data)\n",
    "    d = OrderedDict([(k,c[k]) if k in c else (k,0) for k in x_labels])\n",
    "    # bars are by default width 0.8, so we'll add 0.1 to the left coordinates\n",
    "    bar_width = 0.5\n",
    "    index = np.arange(len(x_labels)) + 0.5\n",
    "    plt.bar(index, d.values(), bar_width)\n",
    "    plt.ylabel('Number of annotations')\n",
    "    plt.axis([0,len(x_labels),y_min,y_max])\n",
    "    plt.title(title)\n",
    "    plt.xticks(index, x_labels)#, rotation='vertical')\n",
    "    plt.show()\n",
    "\n",
    "print('Label distributions for annotators a and b')\n",
    "Y_MIN, Y_MAX = 0, 60\n",
    "make_bar_chart(a_labels, all_labels, Y_MIN, Y_MAX, 'Distribution for annotator a')\n",
    "make_bar_chart(b_labels, all_labels, Y_MIN, Y_MAX, 'Distribution for annotator b')\n",
    "\n",
    "# view confusion matrix\n",
    "key = ['{}={}'.format(i,label) for i,label in enumerate(all_labels)]\n",
    "print('Confusion matrix ({}):\\n'.format(key))\n",
    "_ = plt.matshow(confusion_matrix(a_labels, b_labels), cmap=plt.cm.binary, interpolation='nearest')\n",
    "_ = plt.colorbar()\n",
    "_ = plt.ylabel('annotator a')\n",
    "_ = plt.xlabel('annotator b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating raw agreement and Cohen's Kappa\n",
    "\n",
    "The following are implementations of observed agreement and Cohen's Kappa, a chance-corrected measure of agreement commonly used in Computational Linguistics ([Carletta, 1996](http://www.aclweb.org/anthology/J96-2004))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate observed and expected agreement\n",
    "def observed_agreement(a_labels, b_labels):\n",
    "    \"\"\"Return percentage of instances where we observe that\n",
    "    a_label==b_label\"\"\"\n",
    "    Ao = 0\n",
    "    for a, b in zip(a_labels, b_labels):\n",
    "        if a == b:\n",
    "            Ao += 1\n",
    "    return Ao/len(a_labels)\n",
    "\n",
    "def expected_agreement(a_labels, b_labels):\n",
    "    \"\"\"Return percentage of instances for which we expect\n",
    "    a_label==b_label by chance\"\"\"\n",
    "    a_freqs = Counter(a_labels)\n",
    "    b_freqs = Counter(b_labels)\n",
    "    total = len(a_labels)\n",
    "    Ae = 0\n",
    "    for label in set(a_labels).union(b_labels):\n",
    "        Ae += (a_freqs[label]/total)*(b_freqs[label]/total)\n",
    "    return Ae\n",
    "\n",
    "print('Observed agreement:', observed_agreement(a_labels, b_labels))\n",
    "print('Expected agreement:', expected_agreement(a_labels, b_labels))\n",
    "\n",
    "# Calculate Cohen's Kappa\n",
    "def kappa(a_labels, b_labels):\n",
    "    \"\"\"Calculate Cohen's Kappa\"\"\"\n",
    "    Ao = observed_agreement(a_labels, b_labels)\n",
    "    Ae = expected_agreement(a_labels, b_labels)\n",
    "    return (Ao-Ae)/(1-Ae)\n",
    "\n",
    "print('\\nKappa:', kappa(a_labels, b_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn and NLTK implementations\n",
    "\n",
    "Scikit-learn includes an implementaion of Cohen's kappa. NLTK also implements Kreppendorf's alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Calculate cohen_kappa_score using scikit-learn\n",
    "print('\\nsklearn kappa:', cohen_kappa_score(a_labels, b_labels))\n",
    "\n",
    "# Calculate agreement with NLTK\n",
    "t = AnnotationTask(data=[x.split() for x in open(\"artstein_poesio_example.txt\")])\n",
    "print('\\nnltk kappa:', t.kappa())\n",
    "print('nltk alpha:', t.alpha()) # http://www.aclweb.org/anthology/E14-1058"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining a single gold labelling\n",
    "\n",
    "Given multiple-labelled, we need a way of combining annotations to obtain a single gold labelling. The simplest way to do this is majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "c_labels = ['stat', 'stat', 'chck', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'stat', 'chck', 'ireq', 'ireq', 'stat', 'stat', 'stat', 'ireq', 'ireq', 'ireq', 'ireq', 'ireq', 'ireq', 'ireq', 'ireq', 'stat', 'ireq', 'ireq', 'ireq', 'ireq', 'ireq', 'ireq', 'ireq', 'ireq', 'ireq', 'ireq', 'stat', 'ireq', 'chck', 'stat', 'ireq', 'ireq', 'ireq', 'ireq', 'ireq', 'ireq', 'ireq', 'stat', 'ireq', 'ireq', 'ireq', 'ireq', 'ireq', 'ireq', 'chck', 'chck', 'chck', 'chck', 'stat', 'ireq', 'chck', 'chck', 'chck', 'chck', 'chck', 'chck', 'ireq', 'chck', 'chck']\n",
    "\n",
    "print('Ao(c,a):', observed_agreement(c_labels, a_labels))\n",
    "print('Ao(c,b):', observed_agreement(c_labels, b_labels))\n",
    "print('Kappa(c,a):', kappa(c_labels, a_labels))\n",
    "print('Kappa(c,b):', kappa(c_labels, b_labels))\n",
    "\n",
    "def majority_vote(*args):\n",
    "    for i,labels in enumerate(zip(*args)):\n",
    "        top_2 = Counter(labels).most_common(2)\n",
    "        if len(top_2) == 1:\n",
    "            # all annotators chose the same label\n",
    "            yield top_2[0][0]\n",
    "        elif top_2[0][1] != top_2[1][1]:\n",
    "            # one label is a clear winner\n",
    "            yield top_2[0][0]\n",
    "        else:\n",
    "            # there is a tie\n",
    "            l = random.choice(labels)\n",
    "            warnings.warn('Choosing {} randomly for row {}: {}'.format(l, i,labels))\n",
    "            yield l\n",
    "            \n",
    "gold = list(majority_vote(a_labels, b_labels, c_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted voting\n",
    "\n",
    "- Given a single gold labelling, we can compute human performance according to our evaluation metrics. This provides an upper bound, indicating the best we would expect from a model over this data. Use `sklearn.metrics` to calculate a `precision_score` and a `recall_score` for each annotator against the gold. Then use these to compute average precision and recall.\n",
    "- The above majority voting code generates a warning for row 42, which has a three-way tie with one vote for each label. How could we use Kappa scores to come up with a better voting scheme?\n",
    "- Using Kappa is not an ideal solution. What problems does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 -\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "p_scores, r_scores = [], []\n",
    "for annot_labels in [a_labels, b_labels, c_labels]:\n",
    "    p_scores.append(precision_score(gold, annot_labels, average='micro'))\n",
    "    r_scores.append(recall_score(gold, annot_labels, average='micro'))\n",
    "average_p = sum(p_scores)/len(p_scores)\n",
    "average_r = sum(r_scores)/len(r_scores)\n",
    "f_score = 2*average_p*average_r/(average_p+average_r)\n",
    "print('upper bound p/r/f:', average_p, average_r, f_score)\n",
    "\n",
    "# 2 - Majority vote does not handle cases with no clear winner (ties).\n",
    "#     We could weight each vote by the user's average agreement score.\n",
    "\n",
    "# 3 - Neither majority vote nor weighted voting account for systematic annotator bias.\n",
    "#     See https://lingpipe-blog.com/2014/10/29/beckys-and-my-annotation-paper-in-tacl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
