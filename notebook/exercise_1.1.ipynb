{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 1.1: Annotation & reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Here's a sample class wrapping the main abstraction, the `Dataset`.\n",
    "\n",
    "The following utility reads the 20 newsgroups data into a Dataset object. It sets the label to True if a message comes from the talk.politics.guns group, and to False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "def guns_dataset_factory(subset='train', labelled=False):\n",
    "    \"\"\" Fetches newsgroup data and returns a Dataset. \"\"\"\n",
    "    newsgroups = fetch_20newsgroups(subset=subset)\n",
    "    \n",
    "    # Transform to guns or not.\n",
    "    labels = {i: name == 'talk.politics.guns' for i, name in enumerate(newsgroups.target_names)}\n",
    "    dataset = Dataset({text: labels[i] for text, i in zip(newsgroups.data, newsgroups.target)})\n",
    "    return dataset\n",
    "\n",
    "pool = guns_dataset_factory(subset='train')\n",
    "test = guns_dataset_factory(subset='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A random sampler\n",
    "\n",
    "We need a way of choosing which data to annotate next. Let's start with a random sampler. This is how most crowd annotation is set up.\n",
    "\n",
    "Our `Sampler` base clase includes some utilities for sampling, training and scoring. Other samplers inherit from `Sampler` and must implement the `__call__` method, which takes one argument (a dataset).\n",
    "\n",
    "`Sampler` objects must be initialised with a sklearn classifier or pipeline. We use multinomial naive Bayes as a baseline since it is [fast to train and can achieve competitive accuracy](http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html).\n",
    "\n",
    "Our `Random` sampler simply shuffles the unlabelled data, then returns the first `batch_size` items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from samplers import Random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')),\n",
    "        ('clf', MultinomialNB(alpha=.01)),\n",
    "    ])\n",
    "\n",
    "random_sampler = Random(pipeline, batch_size=10)\n",
    "for i, (text, label) in enumerate(random_sampler(pool)):\n",
    "    print(i, label, repr(text[:60]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated experiments\n",
    "\n",
    "Now we can run simulated experiments. `run_simulation` takes a sampler and runs `n` complete simulations. It returns:\n",
    "* `train_sizes` - the training set sizes for each simulation\n",
    "* `train_scores` - f1 scores over the training set\n",
    "* `test_scores` - f1 scores over the test set\n",
    "\n",
    "Each is a list of tuples, with one tuple per iteration of sampling. Each tuple contains `n` values, one per simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from evaluation import run_simulations\n",
    "\n",
    "# suppress sklearn FutureWarnings in terminal output\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# run a simulated experiment and plot learning curve\n",
    "random_sampler = Random(pipeline, batch_size=2000)\n",
    "train_sizes, train_scores, test_scores = run_simulations(random_sampler, pool, test, n=3)\n",
    "\n",
    "print(train_sizes)\n",
    "print(train_scores)\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves\n",
    "\n",
    "Plotting performance against the number of examples gives a quick visual indication of whether more annotation will help.\n",
    "\n",
    "Here, the lower, green line corresponds to the test f1 score. This is our estimate generalisation to unseen data.\n",
    "\n",
    "When test performance flattens out, more labelled data probably won't help with this classifier (multinomial naive Bayes here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import plot_learning_curve\n",
    "import numpy as np\n",
    "\n",
    "plt = plot_learning_curve(np.asarray(train_sizes), np.asarray(train_scores), np.asarray(test_scores))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation with bootstrap resampling\n",
    "\n",
    "In a setting where sampled batches were actually being annotated, we wouldn't run complete simulations.\n",
    "\n",
    "Instead, we can estimate variance using bootstrap resampling of training data in a given round of sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import run_bootstraps\n",
    "\n",
    "# run a simulated experiment and plot learning curve\n",
    "random_sampler = Random(pipeline, batch_size=2000)\n",
    "train_sizes, train_scores, test_scores = run_bootstraps(random_sampler, pool.copy, test)\n",
    "\n",
    "print(train_sizes)\n",
    "print(train_scores)\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = plot_learning_curve(np.asarray(train_sizes), np.asarray(train_scores), np.asarray(test_scores))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.std(np.asarray(test_scores), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually label some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annotator import AnnotationPane\n",
    "\n",
    "pane = AnnotationPane(pool, Random(pipeline, batch_size=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See our new labels in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pool.label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
